# 引言
## 编写目的

近期我们在梳理经分加载和回传方式时，发现目前整个经分的加载回传情况较往年更加复杂，维护的难度更大。目前公司对经分重视程度提高，要求真正的报表提速，需要在业务和技术层面同时考虑，目前后者考虑存在不足。
1. 按中心经分职责前移的要求，我们对上游的结构进行了了解，发现BOSS资料抽取的网络还存在千兆瓶颈，已经向规划室提出；帐务侧dofile脚本的功能需要深入了解分析。
1. 我们对经分系统内部的数据加载回传过程也进行了分析，发现由于新增加了很多数据库，Greenplum，自助服务库，关键指标B库，加载和回传的需求大大增加；传输脚本仍然沿用db2的传统加载回传脚本，脚本间孤立无法通信，无法统一调度，统一展示监控；目前小型机与x86并存的现状，也造成多套etl，多套加载脚本，数据多次落地，重复处理等情况，整个加载回传过程更加复杂；另外新增加的hadoop的利用率不足，加载速率远没有达到极限；加载过程中既混合了bdi的组件，又存在着bdi调用python脚本、perl脚本，还存在直接使用脚本的情况，在维护、监控和问题处理上更加困难；加载回传调度需要手工逐个配置，耗时耗力。

## 目标
1. 实现全部接口加载/回传过程的可视化，包含接口加载状态，速率。
1. 同类接口使用更高效的新技术来实现，如果可以部分功能考虑使用c语言。
1. 精简脚本，并提供框架实现脚本间的通信和资源的统一调度。
1. 引入gpfs，简化系统结构，减少落地次数，减少数据重复处理。
1. 利用好现有的hadoop，提高加载速度到极限，至少达到2GB/s。
1. 推动上游科室解决经分上游数据传输的网络瓶颈，对于计费dofile尝试理解并进行改造改造，摆脱对统一资料库的依赖，简化代码，实现多进程处理同一目录的功能。

##	经分加载和回传现状描述
图1 经分系统接口传输示意图

<img src="http://133.96.72.138/wordpress/wp-content/uploads/2016/12/interface-300x254.jpg" alt="经分系统接口传输示意图" /> 

经分系统接口传输示意图
分层描述，先说数据源，经分内部的部分，再说加载（蓝线），红线，绿线，能合并的尽量合并。图中蓝线表示从上到下的外部数据源加载到经分内部数据源，红线表示从下到上的经分内部数据源回传给外部数据源，绿线表示经分内部数据源之间的互传。其中，线上标签表示一类数据；Hadoop在中间起中转作用。

1. 基本情况描述：
外部数据源包含网管（10.120.150.129），集团VGOP（133.96.105.196），ERP（10.254.26.134），资产（133.96.47.200），信令（4G:10.216.7.196-201; 2/3G:10.216.9.4-5），BOSS话单（133.96.60.83/84），CRM统一资料库，营业库（接口机133.96.65.95/96），BOMC，客服，CQ，4A接口，运营管控（133.96.72.157），集团一经（10.254.126.134），电渠（10.120.233.3）。 
经分内部数据库包含：GP库，B库，集市，一经，portal库，自助服务库，精准营销库，其中精准是oracle，Gp为greenplum，其它全为db2。
hadoop作为共享存储，保存导入导出的数据文件，但w部分接口数据没有经Hadoop交互。GP库目前承担了原数据仓库的大部分工作，包括接口加载和回传、作业汇总等作业，其他数据库（例如一经、B库、集市库和portal库，自助服务库）的数据高度依赖于GP库。一经库在业务上依赖于Gp，B库。目前有4套ETL在使用：BDI用于GP，一经ETL，B库ETL，集市ETL；目前各数据库间的交互为：Gp-Gp（暂时没有）；db2-db2，db2先导出到本地加载机，再ftp到对端加载机库；GP-db2，Gp先导出到hadoop，再ftp到db2加载机，加载入库。

1. 从外部数据源传递到经分数据库过程
 1. 外部数据源传送到hadoop：网管、集团VGOP、ERP、资产、信令、BOSS话单库的数据传送到Hadoop；CRM统一资料库/营业库的数据由资料接口机（133.96.65.95/96）抽取到本地传送到Hadoop， BOMC、客服类、CQ、4A接口的数据先传送到BDI接口机(133.96.77.175)再传送到Hadoop；
 1.  以上数据从Hadoop以gphdfs方式加载文件（外部表）到GP库；以上数据由Hadoop传送到db2（一经库、B库、集市库）的本地加载机再加载入库。
 1.  存在部分外部数据源到经分数据库不经过Hadoop的情况：集团一经下发的数据直接传送到一经加载机加载；电渠到精准营销的数据先传送到精准营销接口机（133.96.75.214）再加载。

1.  从经分数据库回传给外部数据源
  1. 从经分数据库回传给外部数据源经过hadoop：GP通过gphdfs导出外部表到Hadoop；从Hadoop传到外部数据源：从Hadoop直接回传给网管；从Hadoop先传到BDI接口机(133.96.77.175)再回传给CRM、客服。
  1. 从经分回传给外部数据源中间不经过Hadoop：一经库先导出数据到一经加载机再上传到集团一经接口机；portal库导出数据上传到运营管控接口机。

1.  经分内部数据源之间互传

  1. 从GP库回传：GP回传给DB2数据库（一经库、集市库、关键指标B库、Portal库）和Oracle数据库（精准营销库）都是通过gphdfs导出外部表到Hadoop再ftp传送到本地加载机入库，GP回传给自助服务库是通过gpfdist导出外部表到hadoop后由B库加载机的脚本加载入库。其中GP库回传给一经库主要是垃圾短信、用户标签、校园等数据，GP回传给集市库主要是下放数据用于地市人员临时需求，GP回传给精准营销库主要是标签数据，GP回传给B库主要用于自助分析平台。
  1. 从一经库回传：一经回传给B库和GP库中间都经过Hadoop，其中，一经库回传综合订报类相关数据到Hadoop再传送到B库本地加载机入库；一经库回传客户数流量相关数据到Hadoop再通过gphdfs方式导入GP库。
  1. 从关键指标B库回传：B库回传给GP库星级和竞争对手类等相关数据，B库先导出数据到本地加载机再上传到Hadoop然后通过gphdfs导入外部表到GP库；B库回传给一经库、集市库、Portal库、自助服务库和精准营销库都是先导出数据到本地加载机再传送到各库本地入库。其中，B库回传给一经库主要是家宽、实名制等数据，B库回传给精准营销库主要是标签数据，B库回传给集市库主要是下放数据，B库回传给Portal库主要是印刷报表等数据。
  1. 从集市库回传：集市库回传给Portal库主要是原从兴报表数据，集市库先导出数据到本地加载机再传送给Portal库入库。
  1. 从Portal库回传：Portal库回传给GP库和B库主要是区域化数据，其中，Portal库回传GP库是先导出数据传送到Hadoop然后再通过gphdfs导入外部表到GP库，Portal库回传给B库是先导出数据传送到B库加载机入库。
  1. 从精准营销库回传：精准营销库回传给GP库，精准营销库回传给DB2数据库（集市库、Portal库）先导出数据再传送给各库本地加载入库，其中回传给集市库用于地市人员临时需求，回传给Portal库用于报表前台展示。

##	问题分析
目前加载存在问题如下：
1.	回传给外部数据源应该由Hadoop直接传送给对端主机，不应该中间经过BDI接口机（133.96.77.175）作为中转机落地；
1.	精准营销接口机（133.96.75.214）作为中转机是冗余的，应该两端主机直接传送数据；
1.  集市库不应该回传给portal库数据，因为集市库从规划上不应该有汇总和应用的功能，相关功能应当由GP和portal承担，集市应该只用于地市人员临时需求和统计；
1.  加载回传脚本之间是孤立的，相互之间不能通信，不能协同工作，不能统一调度；
1.  加载机存在着单点问题，加载机一旦出现问题会导致无法工作；
1.  经分内部多个不同数据库的重复下载相同的数据，无法复用，存在数据重复多次落地和处理的情况，尤其是db2-db2的数据交互；
1.	BDI加载脚本只能从一台主机发起对某个接口的加载，单进程串行执行，不支持并发调起多个进程执行同一任务，这种情况效率较低而且容易出现单点问题；
1.  经分内部一经，B库，Gp之间存在数据依赖，耦合严重，如果一个数据库出现问题会直接影响关联的数据库；
1.  GP加载回传既存在使用gphdfs外部表经过Hadoop方式，也存在使用gpfdist外部表落地本地文件系统的方式，统一管理维护很困难；
1.	BDI通过外部表从Hadoop加载没有充分使用网络带宽，现在大概每台主机网卡高峰大概只用600Mb/s，网络利用率有待提高；
1.	通过BDI、一经ETL、集市ETL和B库ETL只能查看单个接口加载情况，无法观察以及监控所有接口的整体加载情况，不能一目了然的知道哪些接口已经加载和哪些接口未加载，对加载情况分析和优化存在困难；
1.	GP回传给集市、Portal库和B库均通过BDI调用外部python脚本导出到Hadoop再从Hadoop直接上传推送到加载机，然后调用外部perl脚本加载入库。但是GP回传给一经是通过BDI调用外部python脚本导出到Hadoop，再由一经ETL及相关脚本加载入库，BDI和一经ETL两个系统无法直接对接，现在靠维护人员凭借经验配置定时任务实现时间对接，这种情况可能出现较大的等待时间窗口以及对接问题；
1.  加载过程中混合使用了BDI组件、BDI调用python脚本、perl脚本和shell脚本等多种加载方式，在监控、维护上很困难；
1.	在用4个ETL系统BDI、一经ETL、集市ETL和B库ETL，维护人员需要同时关注多个ETL系统页面并且不停的切换，多个ETL无法直接对接和兼容。另外，BDI只能用chrome浏览器查看，其他ETL只能使用IE浏览器查看，每个ETL都会启动一个java applet导致维护人员浏览器占用客户端主机内存过多而卡死；


##	改造思路
1. 结构优化：引入GPFS改造目前db2-db2的加载方式，可以节约存储空间；提高加载机的高可用性；提高加载速度；减少数据落地次数，提高设备利用率。
1. 业务优化：减少一经与GP，B库间的业务依赖，重点保障一经考核。
1. 程序优化：最大化的采用目前我们能掌握的rsync（增量传输）、Gearman（分布式处理框架）、perl、python等优化现有脚本达到优化目标；使用GP复制迁移功能实现快速数据装载和卸载；实现通过WEB界面可视化监控传输加载的整个过程，监控单个进程的加载速度，使监控值班人员能够不必登录主机就能够简单、清楚、及时地发现问题或错误，降低监控值班的门槛，减轻值班人员压力；对留存加载日志进行分析，及时发现加载过程中的问题并给予优化；对单个传输进程进行限速，避免过多的进程相互影响，提高网络使用效率；使用Gearman实现分布式并行处理，充分利用网络、磁盘I/O等各种资源，同时能够实现并发数控制，防止过载造成效率下降；使用内存数据库，作为配置库和加载过程记录库，在内存数据库建立配置表、状态表和日志表，全局共享这些表实现信息传递，其一，内存数据库比传统数据库处理速度更快，并发性能更好，更稳定；其二，内存库在业界使用广泛安全众多，网络上有足够的资料可以借鉴；并在以上基础上整合现有的分区合并，表分析回收功能。
## 结构优化
1. 从图及问题分析可以看到，现存的经分加载由于小型机和x86混合，对于db2-db2的加载来说，存在多次ftp的过程，一份数据保存了多份（5），从性能上看接口机和加载机的网络和IO压力都很大，各设备的网络带宽基本达到峰值，传统的IOE设备网络带宽最大只能达到100MB/S，即使使用了多网卡绑定，也无法达到理想的网络速度，各加载机都是单点，保障难度大，一旦发生问题，会严重影响加载的过程。目前db2的时间要求比GP高，但是数据先到GP再到hadoop，加载时间晚。
## 结构调整优化建议
1. 引入Gpfs，连接boss加载机与所有的db2加载机和自助服务库，portal库，使用8台x86设备替换现有小型机设备，统一纳入BDI管理，8台x86中2台作为boss加载机替换现有的加载机，位置放在经分机房（考虑光纤长度），其它4台作为一经加载机和B库加载机，gpfsserver，每台通过两块HBA卡连接连接HP和IBM存储，安装db2分区，最后2台加载机通过hba（或者是lan）连接到gpfs文件系统，用于集市的加载，其它的db2数据库或者加载机通过net client连接到gpfs。所有新增的x86作为BDI的节点统一由BDI管理。
1. 调整后目标：数据在加载过程中，由于数据分别在SAN网络和LAN网络传输，加快了数据传输速度，数据只有一次落地提前了加载时间，避免了现有千兆设备的网络带宽瓶颈，相同的数据只保存了1份，减少一半的存储空间使用。B库、一经只合并一次（考虑是否可以不合并），减少了合并需要的资源和时间,将各库加载时间提前。提高了整个经分加载的高可用，任何一台加载机故障，不影响数据安全，修复主机后，立即可以开始工作，或者加载脚本由其它主机接管。预计可将加载时间提前1小时。加载速度不低于现有情况。
1. 其它问题：GPFS在河北，天津，黑龙江移动业支系统都有使用，在boss侧也有应用，稳定性和效率很可靠，此次我们使用mpio+1的方式，原厂保证使用上不会有问题。在维护侧增加相关实施人天和gpfs许可，实施时由ibm内部协调软件人天完成。


##	概要设计



根据主机操作系统和功能要求的不同，程序主要由3种编程语言开发：在小型机上主要使用perl语言编写加载和回传脚本；在x86服务器上主要使用python语言编写加载和回传脚本；前台展示监控界面主要使用java语言编写。后台数据库使用内存数据库+关系型数据库组合方式实现全局信息共享以及高可用，其中内存数据库用于实时增删改查当前的状态信息，关系型数据库用于持久化历史的状态信息，状态信息主要包括当前任务的进度、是否成功或失败以及报错具体信息等。所有任务在执行过程中都实时更新内存库中当前的状态信息。前台监控界面查询内存库并展示当前的状态信息。对所有加载和回传都用接口编号表示。
系统结构描述：
整个系统由两台物理机组成，每台物理设备上安装一套postgresql，两套postgresql做Ha高可用，每台机器上再加装gearman server，加装redis server，（问题是如果使用redis还需要gearman么？）但后两者不做高可用。postgresql与redis通过redis_fwd做整合，应用可以通过postgresql来操作redis，在保证速度的情况下，减少开发难度。

对于加载和下载部分，要求能实时反映进度。更新到redis中，通过前台刷新。

原始的任务统一保存在postgresql中，由配置生成任务，不同时间的任务是不同的对象。需要考虑使用uuid或者是自增序列。





一个接口可能包含多个数据文件，一个文件对应一个任务，任务包含下载(上传)、校验、合并、加载入库、调起ETL、备份、删除过期文件等一系列动作。
通常整个流程如下：
1. 一个接口的数据文件到达外部数据源接口机后，每个数据文件作为一个任务处理，即先通过ftp下载到GPFS共享文件系统并校验文件大小和文件个数；
1. 如果是话单类接口则在GPFS文件系统上则进行合并，如果是其他类型接口则不进行合并；
1. 所有DB2数据库都是通过load命令读取GPFS共享存储上的数据文件加载入库；
1. 数据文件还通过ftp从GPFS共享存储下载到Hadoop对应的目录，GP通过gphdfs外部表的方式读取Hadoop上的数据文件加载入库；
1. 入库成功后，通过调用ETL的Webservice或者底层API接口的方式从外部调起ETL控制流，启动后续汇总过程；
1. 调起ETL控制流成功后，将数据文件移动到备份目录；
1. 检查备份目录的文件是否过期，将过期文件清理删除。 
上述每个动作失败后都可以重复执行。
每个动作都可能存在5种状态：未开始(null)、开始(begin)、执行中(running)、错误(error)、结束(end)。

例如：A06002话单接口分6个批次(A/B/C/D/E/F)从BOSS传输给经分侧，每个批次对应多个小数据文件，每个小数据文件对应地启动一个任务，在配置表中设置程序的动作，包括：下载、校验、合并、加载、ETL调起、备份等动作。

<img src="http://133.96.72.138/wordpress/wp-content/uploads/2017/02/load.png" />




##   详细设计

按照功能将程序划分成下载模块（包含下载后校验）、合并模块（包含合并后校验）、加载模块、ETL调起模块、备份模块（包括删除过期文件）等，各个模块的具体功能如下：
1. 下载模块：一个接口的数据文件到达数据源接口机后，每个文件启动一个下载任务，并行下载同时限制并行任务个数，在ftp下载时显示实时的速率、进度并更新到状态表，将所有数据文件下载到GPFS共享文件系统。下载成功后，对文件大小和个数按照控制文件进行校验，如果相符则进入下一步处理，否则，删除错误文件重新下载并进行校验直到相符。然后，再从GPFS共享文件系统通过ftp方式下载到Hadoop对应的目录，并对文件大小和个数进行校验。
1. 合并模块：对话单类接口文件在GPFS共享文件系统上进行并行合并，将每个批次话单的小文件合并成若干个大文件。其他类型的接口文件不进行合并。合并完成后，对文件大小和个数进行校验，如果与控制文件不相符则删除后重新合并。合并进度更新到状态表。
1. 加载模块：按照接口对文件加载入库，对于DB2数据库通过load命令读取GPFS共享文件系统的数据文件加载入库，对于GP数据库通过gphdfs外部表的方式读取Hadoop上的数据文件加载入库。加载进度更新到状态表。
1. ETL调起模块：通过调用ETL的Webservice接口或者底层API接口的方式从外部调起ETL控制流，触发后续流程。
1. 备份模块：备份目录按数据日期命名，将加载后的数据文件及时移动到备份目录，并检查备份目录是否有过期文件，如果有，则删除过期文件。

dofile程序在BOSS侧抽取数据完毕后将文件的校验信息插入内存库的表中进行全局共享，在数据传递加载过程中通过读取内存库中的校验信息对文件进行准确性校验。
配置表配置接口信息，按配置表的接口信息再根据数据时间周期生成任务列表。
下载加载程序按照列表执行实际任务，下载程序连接到远程接口机检查是否存在任务中数据文件对应的ok文件，如果存在则调起子进程下载对应的数据文件，下载中临时数据文件为.tmp,下载完成重命名成正式文件名，并校验数据文件大小和个数，如果校验正确则删除对应的ok文件并更新状态表，子进程自动退出结束运行，否则说明boss侧数据文件异常，发送告警信息，值班人员手动处理重新下载及校验。
合并程序检查到下载并校验成功的话单文件，开始对小话单文件进行合并，合并完成后对文件大小和个数进行校验，如果校验正确则更新状态表并写入新校验信息，否则发送告警信息并重新进行合并。如果检查到是非话单的其他接口，则跳过合并。
加载程序检查到话单文件合并完毕或者其他接口文件下载校验完毕，则对数据按照时间周期加载入库，DB2数据库使用load方式加载入库，GP库使用gphdfs通过外部表方式加载入库，入库成功后更新状态表。否则删除当期数据重新加载入库。
ETL调起模块检查到上一个流程执行结束并正确，然后调用webservic接口或者底层API接口，调起成功后更新状态表。否则重新调起。
备份程序检查到数据加载入库完毕，则将加载后的数据文件移动到备份目录，并检查是否有过期数据文件，如果有则删除并更新状态表。
为了不影响下载、合并等任务执行效率，更新状态作为单独的一个任务执行，并且两者之间异步调用。

postgres=# create table public.tb_unit_config(
postgres(#   remote_ip varchar(20),
postgres(#   remote_dir varchar(50),
postgres(#   subject varchar(50),
postgres(#   unit_no varchar(50),
postgres(#   unit_name varchar(50),
postgres(#   cycle varchar(5),
postgres(#   target_database varchar(10),
postgres(#   data_date varchar(15)
postgres(# );
CREATE TABLE
postgres=# insert into public.tb_unit_config values('133.96.65.95','./boss','boss','I01007','资料','D','HEBDB','20170227');
INSERT 0 1
postgres=# insert into public.tb_unit_config values('133.96.65.95','./boss','boss','M05046','未知','M','HEBDB','20170227');
INSERT 0 1
postgres=# select * from public.tb_unit_config;
  remote_ip   | remote_dir | subject | unit_no | unit_name | cycle | target_database | data_date 
--------------+------------+---------+---------+-----------+-------+-----------------+-----------
 133.96.65.95 | ./boss     | boss    | I01007  | 资料      | D     | HEBDB           | 20170227
 133.96.65.95 | ./boss     | boss    | M05046  | 未知      | M     | HEBDB           | 20170227
(2 rows)

postgres=# create table public.tb_unit_check(
postgres(#   file_name varchar(50),
postgres(#   unit_no varchar(50),
postgres(#   create_timestamp timestamp,
postgres(#   file_size varchar(50),
postgres(#   record_count varchar(50)
postgres(# );
CREATE TABLE
postgres=# insert into public.tb_unit_check values('M0504620170228318001.AVL','05046',now(),'1134042548','15277741');
INSERT 0 1
postgres=# insert into public.tb_unit_check values('I0100720170228212001.AVL','01007',now(),'4548176391','12610802');
INSERT 0 1
postgres=# select * from public.tb_unit_check;
        file_name         | unit_no |      create_timestamp      | file_size  | record_count 
--------------------------+---------+----------------------------+------------+--------------
 M0504620170228318001.AVL | 05046   | 2017-03-02 02:13:21.370116 | 1134042548 | 15277741
 I0100720170228212001.AVL | 01007   | 2017-03-02 02:14:58.790064 | 4548176391 | 12610802
(2 rows)
postgres=# insert into public.tb_unit_check values('I0100720170227212001.AVL','01007',now(),'4548176391','12610802');
INSERT 0 1
postgres=# insert into public.tb_unit_check values('I0100720170301212001.AVL','01007',now(),'4548176391','12610802');
INSERT 0 1
postgres=# create table public.tb_unit_task_log(
postgres(#   file_name varchar(50),
postgres(#   state varchar(10),
postgres(#   start_time varchar(20),
postgres(#   end_time varchar(20)
postgres(# );
CREATE TABLE

##	割接时间安排
以下为割接实施初步时间安排，待补充。
##	割接实施步骤
以下为割接实施步骤，待补充。
##	风险及规避措施
新脚本和旧脚本分别是两套独立脚本，使用资源没有交集，不会相互影响。如果遇到新脚本无法处理的错误，可以直接停止新脚本，回退启用旧脚本。
整体测试过程中对各部分割接内容进行测试，若出现问题无法处理的情况下，进行回退操作，避免影响GP库和其他依赖数据库出数。
##	回退方案
本次割接如遇问题或在规定时间内未能完成割接工作，则实施该回退方案。回退步骤待补充。
  
